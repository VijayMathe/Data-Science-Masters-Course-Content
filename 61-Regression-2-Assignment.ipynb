{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ccc004-bf09-4af4-aec7-c1ee524b1307",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d8b1c-dc03-484a-9d26-1ff89c3eef3a",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### The R-Squared is one of the performance matrix used in the linear regression. The R-Square help us to determine the accuracy of the model\n",
    "### The R-Squared can be calculated as 1 - (Sum of squared reciduals/Sum of square total). Where the SSR can be given as the the squared sumation of the difference between the predicted point and the actual point of the n datapoints, the SST can be given as the squared sumation of the difference between the point which is in the aveage line of the y datapoints and the actual point of the n datapoints.\n",
    "### The R-Squared represents the model accuracy\n",
    "\n",
    "### It represents the proportion of the variance in the dependent variable that can be explained by the independent variables in the model.\n",
    "\n",
    "### R-squared is calculated by comparing the variation in the dependent variable explained by the regression model to the total variation in the dependent variable. Mathematically, it can be expressed as:\n",
    "\n",
    "### R-squared = 1 - (SSR / SST)\n",
    "\n",
    "### Where:\n",
    "\n",
    "### SSR (Sum of Squared Residuals) represents the sum of the squared differences between the predicted values by the model and the actual observed values of the dependent variable.\n",
    "### SST (Total Sum of Squares) represents the sum of the squared differences between the observed values of the dependent variable and the mean of the dependent variable.\n",
    "### In simpler terms, R-squared is obtained by dividing the sum of squared residuals (SSR) by the total sum of squares (SST) and subtracting it from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e0c2e-a68e-4d8a-9f33-9a82da42c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a0874b-e394-4a54-b2b3-9394f7b8aaae",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beeb10b-759f-4205-afc8-5a13cbb08c60",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### The adjusted R-squared is always less than the R-Squared, If in the dataset we are adding any of the feature whether the feature is highly correlated with the dependent feature or not the R-Squared will definitly increase.\n",
    "### But the adjusted R-Squared will increase only if the feature which we are adding is highly correlated with the dependent feature othrwise it will decrease\n",
    "### The adjusted R-Squared can be calculated as 1 - ((1 - R2)(N-1))/N-P-1)\n",
    "### where P -> Number of independent features\n",
    "###      N -> Number of datapoints\n",
    "###      R2 -> R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45405987-42b0-41b5-9716-2f6819b3c353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64587e59-faba-4155-adb1-39e9b265287f",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4bbf1-557f-4b7a-912f-6455b4f48637",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### Adjusted R-squared is generally more appropriate to use when comparing regression models with a different number of independent variables or when you want to avoid overfitting. It addresses a limitation of the regular R-squared by penalizing the addition of unnecessary independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcdf62f-701f-413e-bec7-82730bfa9106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dcd9f1e-a967-4915-9bf0-e10e9165513e",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c0b3e-1425-4556-88e6-0bcc66fb70bd",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### In the context of regression analysis, RMSE (Root Mean Square Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of regression models and assess the accuracy of their predictions.\n",
    "### MSE = (1/n) * Σ(y - ŷ)²\n",
    "### RMSE = sqrt(MSE)\n",
    "### MAE = (1/n) * Σ|y - ŷ|\n",
    "### Where: n is the number of data points. \n",
    "### y represents the actual values of the dependent variable.\n",
    "### ŷ represents the predicted values by the regression model.\n",
    "### Both RMSE and MSE are measures that capture the magnitude of the errors, with RMSE being the most commonly used due to its interpretation in the original units of the dependent variable. On the other hand, MAE provides a more interpretable metric as it represents the average absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29078e-e4cb-4fd7-8653-3944960b1788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5859adb-c8cc-4f5c-9217-5b73ebc7c7d5",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d632a60-b897-4c4d-8b4c-e9fdfede846c",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### Advantages:\n",
    "\n",
    "RMSE and MSE both consider the magnitude of errors: RMSE and MSE give more weight to larger errors due to the squaring operation. This can be beneficial in scenarios where larger errors are of particular concern and need to be minimized.\n",
    "\n",
    "RMSE is in the original units of the dependent variable: RMSE retains the same unit of measurement as the dependent variable, making it more interpretable and easier to relate to the problem domain.\n",
    "\n",
    "MAE is robust to outliers: MAE considers the absolute differences between the predicted and actual values, making it less sensitive to outliers. It provides a more robust measure of the average error when the data contains extreme values.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "Sensitivity to outliers: RMSE and MSE give more weight to larger errors, making them sensitive to outliers. Outliers can have a disproportionate impact on these metrics, potentially skewing the evaluation of the model's performance.\n",
    "\n",
    "RMSE and MSE are affected by the scale of the dependent variable: RMSE and MSE are influenced by the scale of the dependent variable. If the dependent variable is on a different scale or unit, it can impact the magnitude of the errors and make comparisons between models or datasets difficult.\n",
    "\n",
    "MAE may not fully capture the model's performance: MAE treats all errors equally without considering the relative importance of individual observations. It may not fully capture the model's performance when certain data points have more significance or require higher precision than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bf518-a92a-47ee-9769-e068b59c73e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af84ddf7-2f32-4b5e-96dc-6ef83c8e0292",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c39beb-4f52-4c67-b4cb-f7ad84a5433a",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### Lasso regularization, also known as L1 regularization, is a technique used in linear regression and other regression models to introduce a penalty term that encourages the model to select only the most important features while shrinking the coefficients of less important features to zero. It helps in feature selection and addresses multicollinearity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a50dda-763d-4161-a073-bc41926aeebc",
   "metadata": {},
   "source": [
    "### Ridge regularization is another statistical technique that is used to prevent overfitting in regression models. It does this by adding a penalty to the model's cost function that is proportional to the sum of the squares of the model's coefficients. This penalty encourages the coefficients to shrink towards zero, but it does not encourage them to shrink to zero as much as Lasso regularization does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368fa41-55a5-4e16-ba24-5145d6aa432b",
   "metadata": {},
   "source": [
    "### Lasso regularization is more appropriate to use when the following conditions are met:\n",
    "\n",
    "### The number of features is large relative to the number of observations.\n",
    "### Some of the features are correlated with each other.\n",
    "### It is important to identify which features are important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29967537-808e-424c-bd29-9c7889b013d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417a71fa-142a-412a-86e1-8541c6aed91a",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b27f4-04fa-413c-9923-1b2ca765a14d",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### Regularized linear models help to prevent overfitting in machine learning by adding a penalty to the model's cost function. This penalty is based on the size of the model's coefficients, and it encourages the model to shrink its coefficients towards zero. This helps to prevent the model from becoming too complex and from fitting the training data too closely. As a result, the model is more likely to generalize well to new data.\n",
    "### Here is an example of how regularized linear models can help to prevent overfitting. Consider a dataset of house prices, where the target variable is the price of the house and the features are the size of the house, the number of bedrooms, and the number of bathrooms. An unregularized linear model might fit a line that passes through all of the data points in the training set. This line would likely be very complex and would fit the training data perfectly. However, this line would also be very likely to overfit the training data and would not generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781d76b-540f-4625-b0da-8669c74fda4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a39f11c-1a75-4f05-95e4-2b472fe03bb0",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ea19a-681f-4098-b8e1-54fb0d92fd9e",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### One limitation of regularized linear models is that they can be computationally expensive to train. This is because the penalty term in the cost function can make the problem non-convex, which can make it difficult to find the global minimum.\n",
    "### Another limitation of regularized linear models is that they can be sensitive to the choice of the regularization parameter. If the regularization parameter is too small, the model may not be able to prevent overfitting. If the regularization parameter is too large, the model may be too simple and may not be able to capture the underlying relationship between the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3dff3-396d-49b9-aafb-74cf4a15138e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81da7959-fb32-4cde-a6ac-f289a04daac0",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfca756-db68-457d-8126-de61d6a710a1",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### I would choose Model A as the better performer. Although Model B has a lower MAE, which means that its predictions are less variable, Model A has a lower RMSE, which means that its predictions are closer to the actual values on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cf636-b649-4fb2-9dc5-056b089ce872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a47282-e556-41f7-a3bd-b00ae6b304ec",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e29426-3d07-4332-950c-aa761ea06ab1",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "In the case of Model A, the regularization parameter is 0.1. This is a relatively low value, which means that ridge regularization will have a relatively small impact on the model. As a result, Model A is likely to be a good performer, but it may not be as robust to overfitting as Model B.\n",
    "\n",
    "In the case of Model B, the regularization parameter is 0.5. This is a relatively high value, which means that lasso regularization will have a significant impact on the model. As a result, Model B is likely to be more robust to overfitting than Model A, but it may also be less interpretable.\n",
    "\n",
    "The choice of regularization method depends on the specific needs of the situation. If the model is being used to make predictions that need to be very accurate, then lasso regularization may be a better choice. However, if the model is being used to make predictions that need to be very interpretable, then ridge regularization may be a better choice.\n",
    "\n",
    "Here are some of the trade-offs and limitations of using regularization methods:\n",
    "\n",
    "Reduced accuracy: Regularization can reduce the accuracy of a model, especially if the regularization parameter is too high.\n",
    "Increased complexity: Regularization can make a model more complex, which can make it more difficult to interpret.\n",
    "Increased computational cost: Regularization can increase the computational cost of training a model.\n",
    "Despite these limitations, regularization methods can be very effective in preventing overfitting and improving the performance of linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09a453-f1f9-4fc5-945d-2912d08368cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
