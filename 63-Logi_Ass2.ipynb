{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2760b9-189d-40ff-8e57-7b21a558d20d",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6c6df-64de-4e60-93eb-07d74a2fc6b0",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### The grid search CV is the hyperparameter tuning technique which is bassically used to find best parameters which will give the best accuracy.\n",
    "### GridSearch CV bassically train the model with each of the possible combination of the features and calculates the accuracy and select that parameter which gives the greater accuracy\n",
    "### GridSearchCV is a hyperparameter tuning technique that is used to find the best parameters for a machine learning model. It does this by training the model with each of the possible combinations of the hyperparameters and then selecting the combination that produces the best accuracy.\n",
    "### GridSearchCV can be a very time-consuming process, especially if there are a large number of hyperparameters to tune. However, it can be a very effective way to find the best parameters for a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae229c-253d-48cf-b9b5-9a48c7b8b3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc0d37b-0846-49e8-b5f5-b587db06c82f",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349cefc-1b39-430e-bdd4-8c2487b3c8e7",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### GridSearchCV and RandomizedSearchCV are both hyperparameter tuning techniques that are used to find the best parameters for a machine learning model. However, there are some key differences between the two methods.\n",
    "\n",
    "### GridSearchCV exhaustively searches through a pre-defined grid of hyperparameter values. This means that it will try every possible combination of hyperparameter values, which can be very time-consuming if there are a large number of hyperparameters to tune. However, GridSearchCV is guaranteed to find the best possible parameters for the model, given the pre-defined grid of hyperparameter values.\n",
    "\n",
    "### RandomizedSearchCV randomly samples hyperparameter values from a pre-defined distribution. This means that it will not try every possible combination of hyperparameter values, which can save time if there are a large number of hyperparameters to tune. However, RandomizedSearchCV is not guaranteed to find the best possible parameters for the model, as it is only sampling from a subset of the possible hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ece71-3064-485c-a6fe-5db2fa7f55e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61fa2e36-f937-41a6-b884-29c0b92af52c",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06dde7-47bd-4639-8e83-3562df76ca97",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### Data leakage is a problem in machine learning that occurs when the model is trained on data that includes information about the target variable. This can lead to the model overfitting the training data and performing poorly on new data.\n",
    "\n",
    "### For example, a model that is trained to predict whether a customer will churn could be overfit if it is trained on data that includes information about the customer's recent spending habits. This is because the model may learn to predict churn based on the customer's spending habits, even though this information is not available at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730cdb3-98ad-4a5a-bfe6-86d73d10404f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd68dd6-1fae-4337-b0f0-c891c97a3d21",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d623a86-dd3a-476b-9b5e-c2f1e0d3bdc6",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### Split the data into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune the hyperparameters of the model, and the test set is used to evaluate the performance of the model. It is important to make sure that the test set is not used for any other purpose, such as data preparation or feature selection.\n",
    "### Use a holdout set. A holdout set is a subset of the data that is not used for any purpose other than evaluating the performance of the model. The holdout set should be kept separate from the training, validation, and test sets.\n",
    "### Use cross-validation. Cross-validation is a technique for evaluating the performance of a machine learning model using multiple folds of the data. This helps to reduce the risk of overfitting, which can be caused by using a single training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef379f-a389-4e24-8786-88cdfb712237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d32e946-9774-46d0-93e3-88981cbc3d69",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a274d02-3d5f-4630-a6b8-12249bd156b3",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### A confusion matrix is a table that is used to evaluate the performance of a classification model. It shows the number of times the model correctly classified the data as well as the number of times it made mistakes. The confusion matrix is divided into four quadrants:\n",
    "\n",
    "True Positive (TP): The model correctly predicted that the data belonged to the positive class.\n",
    "False Positive (FP): The model incorrectly predicted that the data belonged to the positive class.\n",
    "True Negative (TN): The model correctly predicted that the data belonged to the negative class.\n",
    "False Negative (FN): The model incorrectly predicted that the data belonged to the negative class.\n",
    "\n",
    "### The confusion matrix can be used to calculate a number of different metrics to evaluate the performance of the classification model, including:\n",
    "\n",
    "Accuracy: The accuracy of the model is calculated by dividing the number of correct predictions (TP + TN) by the total number of predictions.\n",
    "Precision: Precision is calculated by dividing the number of true positives (TP) by the number of true positives plus the number of false positives (TP + FP).\n",
    "Recall: Recall is calculated by dividing the number of true positives (TP) by the number of true positives plus the number of false negatives (TP + FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97db01c-33b3-40a0-8c52-b32ad78948a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7175cf75-5947-4481-b450-75f834347f20",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce92d37-1674-41fa-b738-714a4b358533",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ans:\n",
    "### Precision = TP / (TP + FP)\n",
    "### Recall = TP / (TP + FN)\n",
    "### Precision measures the proportion of predicted positive instances that are actually positive. It is a measure of how accurate the model is when it predicts positive instances.\n",
    "### Recall measures the proportion of actual positive instances that are predicted positive. It is a measure of how complete the model is when it predicts positive instances.\n",
    "### False positives are instances that are predicted positive but are actually negative. They are a type of overfitting error.\n",
    "### False negatives are instances that are predicted negative but are actually positive. They are a type of underfitting error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c16a02-7308-4fe0-a683-6082f9b0a3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "505a8f32-ee4b-46e3-97c3-e6a895d7bf56",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80abe5-f121-4788-95ca-4db148638706",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### To interpret a confusion matrix to determine which types of errors your model is making, you can look at the following:\n",
    "\n",
    "### True Positives (TP): These are the instances that were correctly predicted as positive. A high number of TPs indicates that the model is good at identifying positive instances.\n",
    "### False Positives (FP): These are the instances that were incorrectly predicted as positive. A high number of FPs indicates that the model is making overfitting errors.\n",
    "### True Negatives (TN): These are the instances that were correctly predicted as negative. A high number of TNs indicates that the model is good at identifying negative instances.\n",
    "### False Negatives (FN): These are the instances that were incorrectly predicted as negative. A high number of FNs indicates that the model is making underfitting errors.\n",
    "\n",
    "### The types of errors that your model is making will depend on the context of the problem. For example, a model that is used to classify spam emails may be more likely to make false positives than false negatives. This is because the cost of a false positive (a legitimate email being classified as spam) is high. On the other hand, a model that is used to diagnose cancer may be more likely to make false negatives than false positives. This is because the cost of a false negative (a cancer patient not being diagnosed) is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edfdc0-fe85-4423-b24c-9ef31d6a0ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "012b5203-6c89-4fbd-8dea-1ef061665572",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855369b-1d55-4f29-9232-c233b6c4f653",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### There are many common metrics that can be derived from a confusion matrix, each of which provides a different perspective on the performance of a classification model. Some of the most common metrics include:\n",
    "\n",
    "### Accuracy: This is the most basic metric, and it is calculated by dividing the number of correct classifications by the total number of classifications.\n",
    "### Precision: This metric measures the proportion of positive classifications that are actually positive. It is calculated by dividing the number of true positives by the sum of the true positives and false positives.\n",
    "### Recall: This metric measures the proportion of positive cases that are correctly identified. It is calculated by dividing the number of true positives by the sum of the true positives and false negatives.\n",
    "### F1 score: This metric is a harmonic mean of precision and recall, and it is often used as a single measure of classification performance. It is calculated by taking 2 times the precision times the recall, divided by the precision plus the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbe933-a38d-4249-b14a-42c608032022",
   "metadata": {},
   "source": [
    "### Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "### Precision = TP / (TP + FP)\n",
    "### Recall = TP / (TP + FN)\n",
    "### F1 score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caeedc2-9359-4725-958e-d04f3f3209df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2018a079-381b-4090-955f-68dceafd612b",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ddbdc-7910-4c1f-a343-6d70fdced553",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "### accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "### where:\n",
    "\n",
    "### TP = True Positives\n",
    "### TN = True Negatives\n",
    "### FP = False Positives\n",
    "### FN = False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331a819-f861-4764-9d14-c5d6dc4f6493",
   "metadata": {},
   "source": [
    "### The accuracy of a model is related to the values in its confusion matrix in the following way:\n",
    "\n",
    "### The accuracy of a model will be high if the number of TP and TN values is high, and the number of FP and FN values is low.\n",
    "### The accuracy of a model will be low if the number of FP and FN values is high, and the number of TP and TN values is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bf597-3359-417d-b103-c0ddee8c5d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c0cc08-4b6c-43fe-ac79-49e2d308f7f9",
   "metadata": {},
   "source": [
    "## Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abc22b-9837-4384-832c-2abd65f3c6af",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a model by showing how many predictions were correct and how many were incorrect. It has four cells, corresponding to the four possible outcomes of a prediction:\n",
    "\n",
    "True Positive (TP): The model predicted that the instance belonged to a class, and it was correct.\n",
    "True Negative (TN): The model predicted that the instance did not belong to a class, and it was correct.\n",
    "False Positive (FP): The model predicted that the instance belonged to a class, but it was incorrect.\n",
    "False Negative (FN): The model predicted that the instance did not belong to a class, but it was incorrect.\n",
    "By looking at the values in the confusion matrix, you can identify potential biases or limitations in your model. For example, if you see that there are a lot of false positives, it means that your model is predicting that an instance belongs to a class when it actually does not. This could be due to bias in the data set, or it could be due to a limitation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604772e-225e-4c7d-aceb-68e78e293e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
